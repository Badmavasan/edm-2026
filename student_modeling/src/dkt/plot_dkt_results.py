#!/usr/bin/env python3
"""Standalone plotting script for DKT results.

Loads CSVs generated by run_dkt_platform_a.py / run_dkt_platform_b.py
and generates plots. No training is performed.

Usage:
    # Plot all (per dataset)
    python src/dkt/plot_dkt_results.py --dataset platform_a
    python src/dkt/plot_dkt_results.py --dataset platform_b

    # Plot hidden-dim comparison for BOTH datasets in one figure
    python src/dkt/plot_dkt_results.py --hidden
"""

from __future__ import annotations

import argparse
import sys
from pathlib import Path

_project_root = str(Path(__file__).resolve().parent.parent.parent)
if _project_root not in sys.path:
    sys.path.insert(0, _project_root)

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from scipy import stats

from src.bkt.plotting import (
    load_and_plot_roc_from_csv,
    plot_auc_multi_modality,
)

sns.set_theme(style="whitegrid", palette="deep")
plt.rcParams["figure.dpi"] = 150

MODALITIES = ["error_independent", "error_dependent"]
LEGEND_LABELS = {
    "error_independent": "Error Independent",
    "error_dependent": "Error Dependent",
}

PLATFORM_LABELS = {
    "platform_a": "Platform A",
    "platform_b": "Platform B",
}

METRICS_TO_PLOT = ["auc", "f1", "precision", "recall"]
METRIC_LABELS = {
    "auc": "AUC",
    "f1": "F1 Score",
    "precision": "Precision",
    "recall": "Recall",
}


def _ensure_parent_dir(path: Path) -> None:
    path.parent.mkdir(parents=True, exist_ok=True)


def plot_hidden_dim_comparison(results_dir: Path, dataset: str) -> None:
    """Plot metrics (mean +/- std error bars) vs hidden_dim per modality (single dataset)."""
    csv_path = results_dir / "hidden_dim_comparison.csv"
    if not csv_path.exists():
        print(f"  Skipping hidden_dim_comparison: {csv_path} not found")
        return

    df = pd.read_csv(csv_path)
    if df.empty:
        print("  Skipping hidden_dim_comparison: CSV is empty")
        return

    fig, axes = plt.subplots(2, 2, figsize=(14, 10), sharex=True)
    axes = axes.flatten()
    colors = {"error_independent": "#669bbc", "error_dependent": "#c1121f"}

    for ax_idx, metric in enumerate(METRICS_TO_PLOT):
        ax = axes[ax_idx]
        mean_col = f"{metric}_mean"
        std_col = f"{metric}_std"

        if mean_col not in df.columns:
            ax.set_visible(False)
            continue

        for modality in MODALITIES:
            mod_df = df[df["modality"] == modality].sort_values("hidden_dim")
            if mod_df.empty:
                continue

            ax.errorbar(
                mod_df["hidden_dim"],
                mod_df[mean_col],
                yerr=mod_df[std_col] if std_col in mod_df.columns else None,
                marker="o",
                linewidth=2,
                markersize=8,
                capsize=5,
                color=colors.get(modality, "gray"),
                label=LEGEND_LABELS.get(modality, modality),
            )

        ax.set_ylabel(f"{METRIC_LABELS[metric]} (mean ± std)", fontsize=11)
        ax.legend(fontsize=9)
        ax.grid(True, alpha=0.3)
        ax.set_ylim([0.4, 1.0])

    for ax in axes[2:]:
        ax.set_xlabel("Hidden Dimension", fontsize=11)

    plt.tight_layout()
    output_path = results_dir / "global" / "hidden_dim_comparison.png"
    _ensure_parent_dir(output_path)
    plt.savefig(output_path, dpi=200)
    plt.close()
    print(f"  Saved: {output_path}")


def plot_hidden_dim_comparison_both(base_path: Path) -> None:
    """
    Plot hidden-dim comparison for BOTH datasets in a single figure:

    - Grid: 4 rows (metrics) x 2 columns (datasets)
    - ONE shared x label + ONE centered legend
    - No overall figure title
    """
    datasets = [("platform_a", PLATFORM_LABELS["platform_a"]), ("platform_b", PLATFORM_LABELS["platform_b"])]

    fig, axes = plt.subplots(
        len(METRICS_TO_PLOT), 2, figsize=(14, 16), sharex=True,
    )

    colors = {"error_independent": "#669bbc", "error_dependent": "#c1121f"}

    # Collect one handle per modality for a single shared legend
    modality_handle = {}

    any_plotted = False
    for col_idx, (dataset, platform_name) in enumerate(datasets):
        results_dir = base_path / "results" / dataset / "dkt"
        csv_path = results_dir / "hidden_dim_comparison.csv"

        df = None
        if csv_path.exists():
            df = pd.read_csv(csv_path)
            if df.empty:
                df = None

        for row_idx, metric in enumerate(METRICS_TO_PLOT):
            ax = axes[row_idx, col_idx]
            mean_col = f"{metric}_mean"
            std_col = f"{metric}_std"

            if row_idx == 0:
                ax.set_title(platform_name, fontsize=13)

            if df is None or mean_col not in df.columns:
                ax.text(
                    0.5, 0.5, "No data",
                    ha="center", va="center", transform=ax.transAxes, fontsize=10,
                )
                ax.grid(True, alpha=0.2)
                continue

            for modality in MODALITIES:
                mod_df = df[df["modality"] == modality].sort_values("hidden_dim")
                if mod_df.empty:
                    continue

                container = ax.errorbar(
                    mod_df["hidden_dim"],
                    mod_df[mean_col],
                    yerr=mod_df[std_col] if std_col in mod_df.columns else None,
                    marker="o",
                    linewidth=2,
                    markersize=7,
                    capsize=5,
                    color=colors.get(modality, "gray"),
                )
                if modality not in modality_handle:
                    modality_handle[modality] = container.lines[0]
                any_plotted = True

            ax.grid(True, alpha=0.3)
            ax.set_ylim([0.4, 1.0])

            # Y-axis label only on the left column
            if col_idx == 0:
                ax.set_ylabel(f"{METRIC_LABELS[metric]} (mean ± std)", fontsize=11)

    if not any_plotted:
        plt.close()
        print("  Skipping combined hidden-dim plot: no data plotted (missing/empty CSVs).")
        return

    # Shared x label
    fig.supxlabel("Hidden Dimension", fontsize=12)

    # One centered legend (modalities only)
    handles = [modality_handle[m] for m in MODALITIES if m in modality_handle]
    labels = [LEGEND_LABELS.get(m, m) for m in MODALITIES if m in modality_handle]
    if handles:
        fig.legend(
            handles,
            labels,
            loc="upper center",
            bbox_to_anchor=(0.5, 1.02),
            ncol=len(handles),
            frameon=False,
            fontsize=10,
        )

    plt.tight_layout(rect=(0, 0, 1, 0.97))

    output_path = base_path / "results" / "dkt" / "global" / "hidden_dim_comparison_platforms.png"
    _ensure_parent_dir(output_path)
    plt.savefig(output_path, dpi=200)
    plt.close()
    print(f"  Saved: {output_path}")


def plot_incremental_auc_samples(results_dir: Path, dataset: str) -> None:
    """Plot AUC vs training samples (both modalities)."""
    data_list = []
    for modality in MODALITIES:
        csv_path = results_dir / "global" / modality / "incremental_training_metrics.csv"
        if csv_path.exists():
            df = pd.read_csv(csv_path)
            if not df.empty and "auc" in df.columns:
                data_list.append({"name": modality, "df": df})

    if not data_list:
        print("  Skipping incremental AUC (samples): no data found")
        return

    output_path = results_dir / "global" / "incremental_training_auc_comparison.png"
    _ensure_parent_dir(output_path)
    plot_auc_multi_modality(
        data_list,
        output_path,
        title=f"DKT: AUC vs Training Data Size ({dataset.title()})",
        xlabel="Training Samples",
        ylabel="AUC",
        legend_labels=LEGEND_LABELS,
        palette="Set2",
        start_from_origin=True,
        figsize=(12, 7),
        legend_loc="lower right",
    )
    print(f"  Saved: {output_path}")


def plot_incremental_auc_students(results_dir: Path, dataset: str) -> None:
    """Plot AUC vs number of training students (both modalities)."""
    data_list = []
    for modality in MODALITIES:
        csv_path = (
            results_dir / "global" / modality / "incremental_training_metrics_by_students.csv"
        )
        if csv_path.exists():
            df = pd.read_csv(csv_path)
            if not df.empty and "auc" in df.columns:
                data_list.append({"name": modality, "df": df})

    if not data_list:
        print("  Skipping incremental AUC (students): no data found")
        return

    output_path = results_dir / "global" / "incremental_training_auc_by_students_comparison.png"
    _ensure_parent_dir(output_path)
    plot_auc_multi_modality(
        data_list,
        output_path,
        title=f"DKT: AUC vs Number of Students ({dataset.title()})",
        xlabel="Number of Training Students",
        ylabel="AUC",
        x_column="train_students",
        legend_labels=LEGEND_LABELS,
        palette="Set2",
        start_from_origin=True,
        figsize=(12, 7),
        legend_loc="lower right",
    )
    print(f"  Saved: {output_path}")


def plot_roc_from_saved(results_dir: Path, dataset: str) -> None:
    """Plot ROC curves from saved roc_data.csv."""
    csv_path = results_dir / "global" / "roc_data.csv"
    if not csv_path.exists():
        # Try combining per-modality files
        all_rows = []
        for modality in MODALITIES:
            mod_csv = results_dir / "global" / modality / "roc_data.csv"
            if mod_csv.exists():
                all_rows.append(pd.read_csv(mod_csv))
        if all_rows:
            combined = pd.concat(all_rows, ignore_index=True)
            _ensure_parent_dir(csv_path)
            combined.to_csv(csv_path, index=False)
        else:
            print("  Skipping ROC plot: no roc_data.csv found")
            return

    output_path = results_dir / "global" / "roc_comparison.png"
    _ensure_parent_dir(output_path)
    load_and_plot_roc_from_csv(csv_path, output_path)
    print(f"  Saved: {output_path}")


def plot_metrics_bar_comparison(results_dir: Path, dataset: str) -> None:
    """Bar chart comparing AUC, F1, Precision, Recall between modalities with significance."""
    fold_data = {}
    for modality in MODALITIES:
        csv_path = results_dir / "global" / modality / "fold_metrics.csv"
        if csv_path.exists():
            df = pd.read_csv(csv_path)
            if not df.empty:
                fold_data[modality] = df

    if len(fold_data) < 2:
        print("  Skipping metrics bar comparison: need data for both modalities")
        return

    fig, ax = plt.subplots(figsize=(10, 6))
    colors = {"error_independent": "#669bbc", "error_dependent": "#c1121f"}

    x = np.arange(len(METRICS_TO_PLOT))
    width = 0.35

    bar_containers = {}
    for i, modality in enumerate(MODALITIES):
        df = fold_data[modality]
        means = [df[m].mean() for m in METRICS_TO_PLOT]
        stds = [df[m].std() for m in METRICS_TO_PLOT]
        offset = -width / 2 + i * width
        bars = ax.bar(
            x + offset, means, width, yerr=stds,
            label=LEGEND_LABELS.get(modality, modality),
            color=colors[modality], capsize=5, alpha=0.85,
            edgecolor="white", linewidth=0.5,
        )
        bar_containers[modality] = bars

    # Add significance annotations from paired t-test
    sig_results = _compute_statistical_tests(fold_data)
    for m_idx, metric in enumerate(METRICS_TO_PLOT):
        if metric in sig_results:
            p_val = sig_results[metric]["p_value"]
            if p_val < 0.001:
                sig_text = "***"
            elif p_val < 0.01:
                sig_text = "**"
            elif p_val < 0.05:
                sig_text = "*"
            else:
                sig_text = "n.s."

            # Draw bracket between the two bars
            y_vals = [
                fold_data[m][metric].mean() + fold_data[m][metric].std()
                for m in MODALITIES
            ]
            y_max = max(y_vals) + 0.02
            x1 = m_idx - width / 2
            x2 = m_idx + width / 2
            ax.plot([x1, x1, x2, x2], [y_max, y_max + 0.015, y_max + 0.015, y_max],
                    color="black", linewidth=1)
            ax.text((x1 + x2) / 2, y_max + 0.018, sig_text,
                    ha="center", va="bottom", fontsize=10, fontweight="bold")

    ax.set_xticks(x)
    ax.set_xticklabels([METRIC_LABELS[m] for m in METRICS_TO_PLOT], fontsize=12)
    ax.set_ylabel("Score (mean ± std across folds)", fontsize=12)
    ax.legend(fontsize=10)
    ax.set_ylim([0, 1.12])
    ax.grid(True, alpha=0.3, axis="y")

    plt.tight_layout()
    output_path = results_dir / "global" / "metrics_comparison.png"
    _ensure_parent_dir(output_path)
    plt.savefig(output_path, dpi=200)
    plt.close()
    print(f"  Saved: {output_path}")


def _compute_statistical_tests(
    fold_data: dict[str, pd.DataFrame],
) -> dict[str, dict]:
    """Run paired t-test between the two modalities for each metric.

    Returns a dict mapping metric name -> {t_stat, p_value, significant}.
    """
    results = {}
    m1, m2 = MODALITIES
    df1, df2 = fold_data[m1], fold_data[m2]

    for metric in METRICS_TO_PLOT:
        if metric not in df1.columns or metric not in df2.columns:
            continue

        vals1 = df1[metric].dropna().values
        vals2 = df2[metric].dropna().values

        # Align by fold count (use minimum length)
        n = min(len(vals1), len(vals2))
        if n < 2:
            continue

        vals1, vals2 = vals1[:n], vals2[:n]
        t_stat, p_value = stats.ttest_rel(vals1, vals2)
        results[metric] = {
            "t_stat": t_stat,
            "p_value": p_value,
            "significant": p_value < 0.05,
            "mean_1": vals1.mean(),
            "std_1": vals1.std(),
            "mean_2": vals2.mean(),
            "std_2": vals2.std(),
            "n_folds": n,
        }

    return results


def perform_statistical_tests(results_dir: Path, dataset: str) -> None:
    """Load fold metrics and perform paired t-tests between modalities.

    Prints a formatted table of results to stdout.
    """
    fold_data = {}
    for modality in MODALITIES:
        csv_path = results_dir / "global" / modality / "fold_metrics.csv"
        if csv_path.exists():
            df = pd.read_csv(csv_path)
            if not df.empty:
                fold_data[modality] = df

    if len(fold_data) < 2:
        print("  Skipping statistical tests: need fold_metrics.csv for both modalities")
        return

    sig_results = _compute_statistical_tests(fold_data)
    if not sig_results:
        print("  No metrics available for statistical testing")
        return

    n_folds = next(iter(sig_results.values()))["n_folds"]
    print(f"\n  Statistical Test: Paired t-test (two-sided, df={n_folds - 1})")
    print(f"  Dataset: {dataset} | Folds: {n_folds}")
    print(f"  Comparing: Error Independent vs Error Dependent")
    print(f"  {'─' * 80}")
    print(
        f"  {'Metric':<12} {'ErrIndep (μ±σ)':>18} {'ErrDep (μ±σ)':>18}"
        f" {'t-stat':>10} {'p-value':>10} {'Significant?':>14}"
    )
    print(f"  {'─' * 80}")

    for metric in METRICS_TO_PLOT:
        if metric not in sig_results:
            continue
        r = sig_results[metric]
        sig_label = "YES *" if r["significant"] else "no"
        if r["p_value"] < 0.01:
            sig_label = "YES **"
        if r["p_value"] < 0.001:
            sig_label = "YES ***"

        print(
            f"  {METRIC_LABELS[metric]:<12}"
            f" {r['mean_1']:.4f}±{r['std_1']:.4f}"
            f" {r['mean_2']:.4f}±{r['std_2']:.4f}"
            f" {r['t_stat']:>10.4f}"
            f" {r['p_value']:>10.4f}"
            f" {sig_label:>14}"
        )

    print(f"  {'─' * 80}")
    print(f"  Significance levels: * p<0.05, ** p<0.01, *** p<0.001")
    print()


def main():
    parser = argparse.ArgumentParser(description="Plot DKT results from saved CSVs")

    group = parser.add_mutually_exclusive_group(required=True)
    group.add_argument(
        "--dataset",
        choices=["platform_a", "platform_b"],
        help="Which dataset results to plot (plots all available figures for that dataset)",
    )
    group.add_argument(
        "--hidden",
        action="store_true",
        help="Plot hidden-dimension comparison for BOTH datasets in one figure",
    )

    args = parser.parse_args()
    base_path = Path(__file__).parent.parent.parent

    if args.hidden:
        print("Plotting hidden dimension comparison for BOTH datasets (Platform A/B)")
        plot_hidden_dim_comparison_both(base_path)
        print("\nDone.")
        return

    # Otherwise: the original per-dataset behavior
    results_dir = base_path / "results" / args.dataset / "dkt"

    if not results_dir.exists():
        print(f"Results directory not found: {results_dir}")
        print("Run the training script first.")
        sys.exit(1)

    print(f"Plotting DKT results for {args.dataset}")
    print(f"Results dir: {results_dir}\n")

    print("1. Hidden dimension comparison (AUC, F1, Precision, Recall)...")
    plot_hidden_dim_comparison(results_dir, args.dataset)

    print("2. Incremental AUC (by samples)...")
    plot_incremental_auc_samples(results_dir, args.dataset)

    print("3. Incremental AUC (by students)...")
    plot_incremental_auc_students(results_dir, args.dataset)

    print("4. ROC curves...")
    plot_roc_from_saved(results_dir, args.dataset)

    print("5. Metrics bar comparison (AUC, F1, Precision, Recall)...")
    plot_metrics_bar_comparison(results_dir, args.dataset)

    print("6. Statistical tests (Error Independent vs Error Dependent)...")
    perform_statistical_tests(results_dir, args.dataset)

    print("Done.")


if __name__ == "__main__":
    main()
